{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k_dwNZXYDY3D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self,pooling_type='max'):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.pooling_type = pooling_type\n",
        "        self.conv1=nn.Conv2d(1,32,kernel_size=2,padding=1)\n",
        "        self.conv2=nn.Conv2d(32,64,kernel_size=2,padding=1)\n",
        "        self.conv3=nn.Conv2d(64,128,kernel_size=2,padding=1)\n",
        "        # self.conv4 = nn.Conv2d(128, 128, kernel_size=2, padding=1)\n",
        "        # self.conv5 = nn.Conv2d(128, 256, kernel_size=2, padding=1)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x=F.relu(self.conv1(x))\n",
        "        x=self._pooling_layer(x)\n",
        "        x=F.relu(self.conv2(x))\n",
        "        x=self._pooling_layer(x)\n",
        "        x=F.relu(self.conv3(x))\n",
        "        x=self._pooling_layer(x)\n",
        "        \n",
        "        #x=F.relu(self.conv4(x))\n",
        "        #x=self._pooling_layer(x)\n",
        "        #x=F.relu(self.conv5(x))\n",
        "        #x=self._pooling_layer(x)\n",
        "        if self.pooling_type=='global_avg':\n",
        "            x=F.adaptive_avg_pool2d(x,(1,1))  \n",
        "            x=x.view(x.size(0),-1)  \n",
        "        else:\n",
        "            x=x.view(x.size(0),-1)  \n",
        "        \n",
        "        return x \n",
        "\n",
        "    def _pooling_layer(self,x):\n",
        "        if self.pooling_type=='max':\n",
        "            return F.max_pool2d(x,2)\n",
        "        elif self.pooling_type=='avg':\n",
        "            return F.avg_pool2d(x,2)\n",
        "        else:\n",
        "            return x  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from CSV\n",
        "def load_fashion_mnist(csv_path):\n",
        "    data=pd.read_csv(csv_path).values  \n",
        "    labels=data[:,0]  \n",
        "    images=data[:,1:] \n",
        "\n",
        "    # Normalize pixel values to [0,1]\n",
        "    images=images.astype(np.float32)/255.0\n",
        "\n",
        "    return images,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:(48000, 784),Validation:(12000, 784),Test:(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Load train and test datasets\n",
        "train_images,train_labels=load_fashion_mnist(\"fashion-mnist_train.csv\")\n",
        "test_images,test_labels=load_fashion_mnist(\"fashion-mnist_test.csv\")\n",
        "\n",
        "# Split train into (train + validation)\n",
        "num_train=int(0.8*train_images.shape[0])\n",
        "val_images,val_labels=train_images[num_train:],train_labels[num_train:]\n",
        "train_images,train_labels =train_images[:num_train],train_labels[:num_train]\n",
        "\n",
        "print(f\"Train:{train_images.shape},Validation:{val_images.shape},Test:{test_images.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(48000, 1, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X_train=train_images.reshape(-1,1,28,28)\n",
        "\n",
        "X_val=val_images.reshape(-1,1,28,28)\n",
        "\n",
        "print(type(X_train),type(X_val),type(train_labels),type(val_labels))\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 of 30, Loss: 2391.4933\n",
            "Epoch 2 of 30, Loss: 1216.3366\n",
            "Epoch 3 of 30, Loss: 938.7772\n",
            "Epoch 4 of 30, Loss: 810.1677\n",
            "Epoch 5 of 30, Loss: 738.0521, Accuracy: 84.43%\n",
            "Epoch 6 of 30, Loss: 693.3818\n",
            "Epoch 7 of 30, Loss: 655.8138\n",
            "Epoch 8 of 30, Loss: 626.5030\n",
            "Epoch 9 of 30, Loss: 604.2284\n",
            "Epoch 10 of 30, Loss: 581.2604, Accuracy: 87.07%\n",
            "Epoch 11 of 30, Loss: 565.2831\n",
            "Epoch 12 of 30, Loss: 552.7113\n",
            "Epoch 13 of 30, Loss: 539.4307\n",
            "Epoch 14 of 30, Loss: 526.6567\n",
            "Epoch 15 of 30, Loss: 513.3921, Accuracy: 87.69%\n",
            "Epoch 16 of 30, Loss: 504.5347\n",
            "Epoch 17 of 30, Loss: 496.7303\n",
            "Epoch 18 of 30, Loss: 488.7234\n",
            "Epoch 19 of 30, Loss: 477.2020\n",
            "Epoch 20 of 30, Loss: 474.8606, Accuracy: 87.96%\n",
            "Epoch 21 of 30, Loss: 472.1863\n",
            "Epoch 22 of 30, Loss: 457.6348\n",
            "Epoch 23 of 30, Loss: 452.6832\n",
            "Epoch 24 of 30, Loss: 448.0309\n",
            "Epoch 25 of 30, Loss: 442.9990, Accuracy: 88.48%\n",
            "Epoch 26 of 30, Loss: 442.3084\n",
            "Epoch 27 of 30, Loss: 432.2115\n",
            "Epoch 28 of 30, Loss: 425.9572\n",
            "Epoch 29 of 30, Loss: 423.5579\n",
            "Epoch 30 of 30, Loss: 416.7970, Accuracy: 88.63%\n"
          ]
        }
      ],
      "source": [
        "x_train=X_train.reshape(X_train.shape[0],-1)\n",
        "x_test=X_val.reshape(X_val.shape[0],-1)\n",
        "\n",
        "y_train=train_labels\n",
        "y_test=val_labels\n",
        "\n",
        "def one_hot_encode(y,num_classes=10):\n",
        "    one_hot=np.zeros((y.shape[0],num_classes))\n",
        "    one_hot[np.arange(y.shape[0]),y]=1\n",
        "    return one_hot\n",
        "y_train=one_hot_encode(y_train)\n",
        "y_test=one_hot_encode(y_test)\n",
        "\n",
        "input_size=784\n",
        "hidden_size1=128\n",
        "hidden_size2=64\n",
        "output_size=10\n",
        "learning_rate=0.03\n",
        "epochs=30\n",
        "batch_size=32\n",
        "dropout_rate=0.2\n",
        "\n",
        "np.random.seed(42)\n",
        "W1=np.random.randn(input_size,hidden_size1)*0.01\n",
        "b1=np.zeros((1,hidden_size1))\n",
        "W2=np.random.randn(hidden_size1,hidden_size2)*0.01\n",
        "b2=np.zeros((1,hidden_size2))\n",
        "W3=np.random.randn(hidden_size2,output_size)*0.01\n",
        "b3=np.zeros((1,output_size))\n",
        "\n",
        "# Activation functions\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return np.where(Z>0,1,0)\n",
        "\n",
        "def leaky_relu(Z,alpha=0.01):\n",
        "    return np.where(Z>0,Z,alpha*Z)\n",
        "\n",
        "def leaky_relu_derivative(Z,alpha=0.01):\n",
        "    return np.where(Z>0,1,alpha)\n",
        "\n",
        "def tanh(Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def tanh_derivative(Z):\n",
        "    return 1-np.tanh(Z)**2\n",
        "\n",
        "def gelu(Z):\n",
        "    return 0.5*Z*(1+np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3)))\n",
        "\n",
        "def gelu_derivative(Z):\n",
        "    return 0.5*(1+np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3)))+(Z*(1-np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3))**2)*(np.sqrt(2/np.pi)*(1+3*0.044715*Z**2))*0.5)\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ=np.exp(Z-np.max(Z,axis=1,keepdims=True))\n",
        "    return expZ/np.sum(expZ,axis=1,keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(Y_true,Y_pred):\n",
        "    return -np.mean(np.sum(Y_true*np.log(Y_pred+1e-9),axis=1))\n",
        "\n",
        "activation_function=relu\n",
        "activation_derivative=relu_derivative\n",
        "\n",
        "def forward_pass(X,dropout_rate=0.2):\n",
        "    global mask1,mask2\n",
        "    Z1=np.matmul(X,W1)+b1\n",
        "    A1=activation_function(Z1)\n",
        "    mask1=(np.random.rand(*A1.shape)>dropout_rate)\n",
        "    A1*=mask1\n",
        "    A1/=(1-dropout_rate)\n",
        "    Z2=np.matmul(A1,W2)+b2\n",
        "    A2=activation_function(Z2)\n",
        "    mask2=(np.random.rand(*A2.shape)>dropout_rate)\n",
        "    A2*=mask2\n",
        "    A2/=(1-dropout_rate)\n",
        "    Z3=np.matmul(A2,W3)+b3\n",
        "    A3=softmax(Z3)\n",
        "    return Z1,A1,Z2,A2,Z3,A3\n",
        "\n",
        "def backward_pass(X,Y,Z1,A1,Z2,A2,Z3,A3,dropout_rate=0.2):\n",
        "    global W1,b1,W2,b2,W3,b3\n",
        "    dZ3=A3-Y\n",
        "    dW3=np.matmul(A2.T,dZ3)/X.shape[0]\n",
        "    db3=np.mean(dZ3, axis=0, keepdims=True)\n",
        "    dA2=np.matmul(dZ3, W3.T)\n",
        "    dA2*=mask2\n",
        "    dA2/=(1-dropout_rate)\n",
        "    dZ2=dA2*activation_derivative(Z2)\n",
        "    dW2=np.matmul(A1.T,dZ2)/X.shape[0]\n",
        "    db2=np.mean(dZ2,axis=0,keepdims=True)\n",
        "    dA1=np.matmul(dZ2,W2.T)\n",
        "    dA1*=mask1\n",
        "    dA1/=(1-dropout_rate)\n",
        "    dZ1=dA1*activation_derivative(Z1)\n",
        "    dW1=np.matmul(X.T,dZ1)/X.shape[0]\n",
        "    db1=np.mean(dZ1,axis=0,keepdims=True)\n",
        "    W1-=learning_rate*dW1\n",
        "    b1-=learning_rate*db1\n",
        "    W2-=learning_rate*dW2\n",
        "    b2-=learning_rate*db2\n",
        "    W3-=learning_rate*dW3\n",
        "    b3-=learning_rate*db3\n",
        "\n",
        "loss_array=[]\n",
        "prev_loss=float('inf')\n",
        "accuracy_array=[]\n",
        "\n",
        "def predict(X):\n",
        "    _,_,_,_,_,A3=forward_pass(X,dropout_rate=0)\n",
        "    return np.argmax(A3,axis=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    indices=np.random.permutation(x_train.shape[0])\n",
        "    x_train_shuffled=x_train[indices]\n",
        "    y_train_shuffled=y_train[indices]\n",
        "\n",
        "    epoch_loss=0\n",
        "    for i in range(0,x_train.shape[0],batch_size):\n",
        "        X_batch=x_train_shuffled[i:i+batch_size]\n",
        "        Y_batch=y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        Z1,A1,Z2,A2,Z3,A3=forward_pass(X_batch,dropout_rate)\n",
        "        loss=cross_entropy_loss(Y_batch,A3)\n",
        "        backward_pass(X_batch,Y_batch,Z1,A1,Z2,A2,Z3,A3,dropout_rate)\n",
        "        epoch_loss+=loss\n",
        "\n",
        "    loss_array.append(epoch_loss)\n",
        "    if epoch_loss>prev_loss:\n",
        "        learning_rate/=2\n",
        "        print(\"Learning decreased\")\n",
        "    prev_loss=epoch_loss\n",
        "    if (epoch+1)%5==0:\n",
        "        y_pred=predict(x_test)\n",
        "        y_true=np.argmax(y_test,axis=1)\n",
        "        accuracy=np.mean(y_pred==y_true)\n",
        "        accuracy_array.append(accuracy)\n",
        "        print(f\"Epoch {epoch+1} of {epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1} of {epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted features shape: torch.Size([48000, 2048])\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "cnn_model=CNNModel(pooling_type='max') \n",
        "X_train_1=torch.from_numpy(X_train).float()\n",
        "X_val_1=torch.from_numpy(X_val).float()\n",
        "features_train=cnn_model(X_train_1)\n",
        "features_val=cnn_model(X_val_1)\n",
        "\n",
        "print(\"Extracted features shape:\",features_train.shape)\n",
        "\n",
        "print(type(features_train))\n",
        "print(type(features_val))\n",
        "print(type(train_labels))\n",
        "print(type(val_labels))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(48000, 2048)\n",
            "Epoch 1 of 30, Loss: 3440.8682\n",
            "Epoch 2 of 30, Loss: 2071.6564\n",
            "Epoch 3 of 30, Loss: 1554.3714\n",
            "Epoch 4 of 30, Loss: 1324.2155\n",
            "Epoch 5 of 30, Loss: 1157.9504, Accuracy: 71.83%\n",
            "Epoch 6 of 30, Loss: 1054.7298\n",
            "Epoch 7 of 30, Loss: 978.7489\n",
            "Epoch 8 of 30, Loss: 928.8942\n",
            "Epoch 9 of 30, Loss: 889.9476\n",
            "Epoch 10 of 30, Loss: 858.0307, Accuracy: 79.97%\n",
            "Epoch 11 of 30, Loss: 838.6723\n",
            "Epoch 12 of 30, Loss: 817.1943\n",
            "Epoch 13 of 30, Loss: 797.7606\n",
            "Epoch 14 of 30, Loss: 772.8477\n",
            "Epoch 15 of 30, Loss: 764.9737, Accuracy: 81.60%\n",
            "Epoch 16 of 30, Loss: 754.6891\n",
            "Epoch 17 of 30, Loss: 742.8750\n",
            "Epoch 18 of 30, Loss: 731.5261\n",
            "Epoch 19 of 30, Loss: 715.4915\n",
            "Epoch 20 of 30, Loss: 711.5083, Accuracy: 83.69%\n",
            "Epoch 21 of 30, Loss: 701.0246\n",
            "Epoch 22 of 30, Loss: 688.1063\n",
            "Epoch 23 of 30, Loss: 682.8511\n",
            "Epoch 24 of 30, Loss: 671.3065\n",
            "Epoch 25 of 30, Loss: 663.9042, Accuracy: 85.48%\n",
            "Epoch 26 of 30, Loss: 659.5959\n",
            "Epoch 27 of 30, Loss: 653.9070\n",
            "Epoch 28 of 30, Loss: 648.4214\n",
            "Epoch 29 of 30, Loss: 641.3444\n",
            "Epoch 30 of 30, Loss: 637.2763, Accuracy: 84.80%\n"
          ]
        }
      ],
      "source": [
        "np_1,np_2=features_train,features_val\n",
        "x_train,y_train,x_test,y_test=np_1.cpu().detach().numpy(),train_labels,np_2.cpu().detach().numpy(),val_labels\n",
        "x_train,x_test =x_train/x_train.max(),x_test/x_test.max()\n",
        "x_train=x_train.reshape(x_train.shape[0],-1)\n",
        "x_test=x_test.reshape(x_test.shape[0],-1)\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "def one_hot_encode(y,num_classes=10):\n",
        "    one_hot=np.zeros((y.shape[0],num_classes))\n",
        "    one_hot[np.arange(y.shape[0]),y]=1\n",
        "    return one_hot\n",
        "y_train=one_hot_encode(y_train)\n",
        "y_test=one_hot_encode(y_test)\n",
        "\n",
        "input_size=2048\n",
        "hidden_size1=128\n",
        "hidden_size2=64\n",
        "output_size=10\n",
        "learning_rate=0.03\n",
        "epochs=30\n",
        "batch_size=32\n",
        "dropout_rate=0.2\n",
        "\n",
        "np.random.seed(42)\n",
        "W1=np.random.randn(input_size,hidden_size1)*0.01\n",
        "b1=np.zeros((1,hidden_size1))\n",
        "W2=np.random.randn(hidden_size1,hidden_size2)*0.01\n",
        "b2=np.zeros((1,hidden_size2))\n",
        "W3=np.random.randn(hidden_size2,output_size)*0.01\n",
        "b3=np.zeros((1,output_size))\n",
        "\n",
        "# Activation functions\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return np.where(Z>0,1,0)\n",
        "\n",
        "def leaky_relu(Z,alpha=0.01):\n",
        "    return np.where(Z>0,Z,alpha*Z)\n",
        "\n",
        "def leaky_relu_derivative(Z,alpha=0.01):\n",
        "    return np.where(Z>0,1,alpha)\n",
        "\n",
        "def tanh(Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def tanh_derivative(Z):\n",
        "    return 1-np.tanh(Z)**2\n",
        "\n",
        "def gelu(Z):\n",
        "    return 0.5*Z*(1+np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3)))\n",
        "\n",
        "def gelu_derivative(Z):\n",
        "    return 0.5*(1+np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3)))+(Z*(1-np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3))**2)*(np.sqrt(2/np.pi)*(1+3*0.044715*Z**2))*0.5)\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ=np.exp(Z-np.max(Z,axis=1,keepdims=True))\n",
        "    return expZ/np.sum(expZ,axis=1,keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(Y_true,Y_pred):\n",
        "    return -np.mean(np.sum(Y_true*np.log(Y_pred+1e-9),axis=1))\n",
        "\n",
        "activation_function=relu\n",
        "activation_derivative=relu_derivative\n",
        "\n",
        "def forward_pass(X,dropout_rate=0.2):\n",
        "    global mask1,mask2\n",
        "    Z1=np.matmul(X,W1)+b1\n",
        "    A1=activation_function(Z1)\n",
        "    mask1=(np.random.rand(*A1.shape)>dropout_rate)\n",
        "    A1*=mask1\n",
        "    A1/=(1-dropout_rate)\n",
        "    Z2=np.matmul(A1,W2)+b2\n",
        "    A2=activation_function(Z2)\n",
        "    mask2=(np.random.rand(*A2.shape)>dropout_rate)\n",
        "    A2*=mask2\n",
        "    A2/=(1-dropout_rate)\n",
        "    Z3=np.matmul(A2,W3)+b3\n",
        "    A3=softmax(Z3)\n",
        "    return Z1,A1,Z2,A2,Z3,A3\n",
        "\n",
        "def backward_pass(X,Y,Z1,A1,Z2,A2,Z3,A3,dropout_rate=0.2):\n",
        "    global W1,b1,W2,b2,W3,b3\n",
        "    dZ3=A3-Y\n",
        "    dW3=np.matmul(A2.T,dZ3)/X.shape[0]\n",
        "    db3=np.mean(dZ3, axis=0, keepdims=True)\n",
        "    dA2=np.matmul(dZ3, W3.T)\n",
        "    dA2*=mask2\n",
        "    dA2/=(1-dropout_rate)\n",
        "    dZ2=dA2*activation_derivative(Z2)\n",
        "    dW2=np.matmul(A1.T,dZ2)/X.shape[0]\n",
        "    db2=np.mean(dZ2,axis=0,keepdims=True)\n",
        "    dA1=np.matmul(dZ2,W2.T)\n",
        "    dA1*=mask1\n",
        "    dA1/=(1-dropout_rate)\n",
        "    dZ1=dA1*activation_derivative(Z1)\n",
        "    dW1=np.matmul(X.T,dZ1)/X.shape[0]\n",
        "    db1=np.mean(dZ1,axis=0,keepdims=True)\n",
        "    W1-=learning_rate*dW1\n",
        "    b1-=learning_rate*db1\n",
        "    W2-=learning_rate*dW2\n",
        "    b2-=learning_rate*db2\n",
        "    W3-=learning_rate*dW3\n",
        "    b3-=learning_rate*db3\n",
        "\n",
        "loss_array=[]\n",
        "prev_loss=float('inf')\n",
        "accuracy_array=[]\n",
        "\n",
        "def predict(X):\n",
        "    _,_,_,_,_,A3=forward_pass(X,dropout_rate=0)\n",
        "    return np.argmax(A3,axis=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    indices=np.random.permutation(x_train.shape[0])\n",
        "    x_train_shuffled=x_train[indices]\n",
        "    y_train_shuffled=y_train[indices]\n",
        "\n",
        "    epoch_loss=0\n",
        "    for i in range(0,x_train.shape[0],batch_size):\n",
        "        X_batch=x_train_shuffled[i:i+batch_size]\n",
        "        Y_batch=y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        Z1,A1,Z2,A2,Z3,A3=forward_pass(X_batch,dropout_rate)\n",
        "        loss=cross_entropy_loss(Y_batch,A3)\n",
        "        backward_pass(X_batch,Y_batch,Z1,A1,Z2,A2,Z3,A3,dropout_rate)\n",
        "        epoch_loss+=loss\n",
        "\n",
        "    loss_array.append(epoch_loss)\n",
        "    if epoch_loss>prev_loss:\n",
        "        learning_rate/=2\n",
        "        print(\"Learning decreased\")\n",
        "    prev_loss=epoch_loss\n",
        "    if (epoch+1)%5==0:\n",
        "        y_pred=predict(x_test)\n",
        "        y_true=np.argmax(y_test,axis=1)\n",
        "        accuracy=np.mean(y_pred==y_true)\n",
        "        accuracy_array.append(accuracy)\n",
        "        print(f\"Epoch {epoch+1} of {epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1} of {epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# ax1.plot(range(1, epochs + 1), loss_array, color='orange', linestyle='-', linewidth=1, label='Loss')\n",
        "# ax1.set_xlabel('Epochs')\n",
        "# ax1.set_ylabel('Loss', color='orange')\n",
        "# ax1.tick_params(axis='y')\n",
        "\n",
        "# ax2 = ax1.twinx()\n",
        "# ax2.plot(range(5, epochs + 1, 5), accuracy_array, color='blue', linestyle='--', linewidth=1, label='Accuracy')\n",
        "# ax2.set_ylabel('Accuracy', color='blue')\n",
        "# ax2.tick_params(axis='y')\n",
        "\n",
        "# plt.title('Training Loss and Test Accuracy')\n",
        "# fig.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
