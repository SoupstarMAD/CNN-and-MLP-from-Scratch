{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k_dwNZXYDY3D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset from CSV\n",
        "def load_fashion_mnist(csv_path):\n",
        "    data=pd.read_csv(csv_path).values  \n",
        "    labels=data[:,0]  \n",
        "    images=data[:,1:] \n",
        "\n",
        "    # Normalize pixel values to [0,1]\n",
        "    images=images.astype(np.float32)/255.0\n",
        "\n",
        "    return images,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train:(48000, 784),Validation:(12000, 784),Test:(10000, 784)\n"
          ]
        }
      ],
      "source": [
        "# Load train and test datasets\n",
        "train_images,train_labels=load_fashion_mnist(\"dataset\\\\fashion-mnist_train.csv\")\n",
        "test_images,test_labels=load_fashion_mnist(\"dataset\\\\fashion-mnist_test.csv\")\n",
        "\n",
        "# Split train into (train + validation)\n",
        "num_train=int(0.8*train_images.shape[0])\n",
        "val_images,val_labels=train_images[num_train:],train_labels[num_train:]\n",
        "train_images,train_labels =train_images[:num_train],train_labels[:num_train]\n",
        "\n",
        "print(f\"Train:{train_images.shape},Validation:{val_images.shape},Test:{test_images.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(48000, 1, 28, 28)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "X_train=train_images.reshape(-1,1,28,28)\n",
        "\n",
        "X_val=val_images.reshape(-1,1,28,28)\n",
        "\n",
        "print(type(X_train),type(X_val),type(train_labels),type(val_labels))\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 of 30, Loss: 2391.4933\n",
            "Epoch 2 of 30, Loss: 1216.3366\n",
            "Epoch 3 of 30, Loss: 938.7772\n",
            "Epoch 4 of 30, Loss: 810.1677\n",
            "Epoch 5 of 30, Loss: 738.0521, Accuracy: 84.43%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m y_train_shuffled\u001b[38;5;241m=\u001b[39my_train[indices]\n\u001b[0;32m    119\u001b[0m epoch_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    121\u001b[0m     X_batch\u001b[38;5;241m=\u001b[39mx_train_shuffled[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m    122\u001b[0m     Y_batch\u001b[38;5;241m=\u001b[39my_train_shuffled[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "x_train=X_train.reshape(X_train.shape[0],-1)\n",
        "x_test=X_val.reshape(X_val.shape[0],-1)\n",
        "\n",
        "y_train=train_labels\n",
        "y_test=val_labels\n",
        "\n",
        "def one_hot_encode(y,num_classes=10):\n",
        "    one_hot=np.zeros((y.shape[0],num_classes))\n",
        "    one_hot[np.arange(y.shape[0]),y]=1\n",
        "    return one_hot\n",
        "y_train=one_hot_encode(y_train)\n",
        "y_test=one_hot_encode(y_test)\n",
        "\n",
        "input_size=784\n",
        "hidden_size1=128\n",
        "hidden_size2=64\n",
        "output_size=10\n",
        "learning_rate=0.03\n",
        "epochs=30\n",
        "batch_size=32\n",
        "dropout_rate=0.2\n",
        "\n",
        "np.random.seed(42)\n",
        "W1=np.random.randn(input_size,hidden_size1)*0.01\n",
        "b1=np.zeros((1,hidden_size1))\n",
        "W2=np.random.randn(hidden_size1,hidden_size2)*0.01\n",
        "b2=np.zeros((1,hidden_size2))\n",
        "W3=np.random.randn(hidden_size2,output_size)*0.01\n",
        "b3=np.zeros((1,output_size))\n",
        "\n",
        "# Activation functions\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return np.where(Z>0,1,0)\n",
        "\n",
        "def leaky_relu(Z,alpha=0.01):\n",
        "    return np.where(Z>0,Z,alpha*Z)\n",
        "\n",
        "def leaky_relu_derivative(Z,alpha=0.01):\n",
        "    return np.where(Z>0,1,alpha)\n",
        "\n",
        "def tanh(Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def tanh_derivative(Z):\n",
        "    return 1-np.tanh(Z)**2\n",
        "\n",
        "def gelu(Z):\n",
        "    return 0.5*Z*(1+np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3)))\n",
        "\n",
        "def gelu_derivative(Z):\n",
        "    return 0.5*(1+np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3)))+(Z*(1-np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3))**2)*(np.sqrt(2/np.pi)*(1+3*0.044715*Z**2))*0.5)\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ=np.exp(Z-np.max(Z,axis=1,keepdims=True))\n",
        "    return expZ/np.sum(expZ,axis=1,keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(Y_true,Y_pred):\n",
        "    return -np.mean(np.sum(Y_true*np.log(Y_pred+1e-9),axis=1))\n",
        "\n",
        "activation_function=relu\n",
        "activation_derivative=relu_derivative\n",
        "\n",
        "def forward_pass(X,dropout_rate=0.2):\n",
        "    global mask1,mask2\n",
        "    Z1=np.matmul(X,W1)+b1\n",
        "    A1=activation_function(Z1)\n",
        "    mask1=(np.random.rand(*A1.shape)>dropout_rate)\n",
        "    A1*=mask1\n",
        "    A1/=(1-dropout_rate)\n",
        "    Z2=np.matmul(A1,W2)+b2\n",
        "    A2=activation_function(Z2)\n",
        "    mask2=(np.random.rand(*A2.shape)>dropout_rate)\n",
        "    A2*=mask2\n",
        "    A2/=(1-dropout_rate)\n",
        "    Z3=np.matmul(A2,W3)+b3\n",
        "    A3=softmax(Z3)\n",
        "    return Z1,A1,Z2,A2,Z3,A3\n",
        "\n",
        "def backward_pass(X,Y,Z1,A1,Z2,A2,Z3,A3,dropout_rate=0.2):\n",
        "    global W1,b1,W2,b2,W3,b3\n",
        "    dZ3=A3-Y\n",
        "    dW3=np.matmul(A2.T,dZ3)/X.shape[0]\n",
        "    db3=np.mean(dZ3, axis=0, keepdims=True)\n",
        "    dA2=np.matmul(dZ3, W3.T)\n",
        "    dA2*=mask2\n",
        "    dA2/=(1-dropout_rate)\n",
        "    dZ2=dA2*activation_derivative(Z2)\n",
        "    dW2=np.matmul(A1.T,dZ2)/X.shape[0]\n",
        "    db2=np.mean(dZ2,axis=0,keepdims=True)\n",
        "    dA1=np.matmul(dZ2,W2.T)\n",
        "    dA1*=mask1\n",
        "    dA1/=(1-dropout_rate)\n",
        "    dZ1=dA1*activation_derivative(Z1)\n",
        "    dW1=np.matmul(X.T,dZ1)/X.shape[0]\n",
        "    db1=np.mean(dZ1,axis=0,keepdims=True)\n",
        "    W1-=learning_rate*dW1\n",
        "    b1-=learning_rate*db1\n",
        "    W2-=learning_rate*dW2\n",
        "    b2-=learning_rate*db2\n",
        "    W3-=learning_rate*dW3\n",
        "    b3-=learning_rate*db3\n",
        "\n",
        "loss_array=[]\n",
        "prev_loss=float('inf')\n",
        "accuracy_array=[]\n",
        "\n",
        "def predict(X):\n",
        "    _,_,_,_,_,A3=forward_pass(X,dropout_rate=0)\n",
        "    return np.argmax(A3,axis=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    indices=np.random.permutation(x_train.shape[0])\n",
        "    x_train_shuffled=x_train[indices]\n",
        "    y_train_shuffled=y_train[indices]\n",
        "\n",
        "    epoch_loss=0\n",
        "    for i in range(0,x_train.shape[0],batch_size):\n",
        "        X_batch=x_train_shuffled[i:i+batch_size]\n",
        "        Y_batch=y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        Z1,A1,Z2,A2,Z3,A3=forward_pass(X_batch,dropout_rate)\n",
        "        loss=cross_entropy_loss(Y_batch,A3)\n",
        "        backward_pass(X_batch,Y_batch,Z1,A1,Z2,A2,Z3,A3,dropout_rate)\n",
        "        epoch_loss+=loss\n",
        "\n",
        "    loss_array.append(epoch_loss)\n",
        "    if epoch_loss>prev_loss:\n",
        "        learning_rate/=2\n",
        "        print(\"Learning decreased\")\n",
        "    prev_loss=epoch_loss\n",
        "    if (epoch+1)%5==0:\n",
        "        y_pred=predict(x_test)\n",
        "        y_true=np.argmax(y_test,axis=1)\n",
        "        accuracy=np.mean(y_pred==y_true)\n",
        "        accuracy_array.append(accuracy)\n",
        "        print(f\"Epoch {epoch+1} of {epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1} of {epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes=10, pooling_type='max'):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.pooling_type = pooling_type\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.feature_size = 64 * 7 * 7  \n",
        "        \n",
        "        self.fc = nn.Linear(self.feature_size, num_classes)  \n",
        "\n",
        "    def forward(self, x, extract_features=False):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self._pooling_layer(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self._pooling_layer(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1) \n",
        "\n",
        "        if extract_features:\n",
        "            return x  \n",
        "        \n",
        "        x = self.fc(x)  \n",
        "        return x\n",
        "\n",
        "    def _pooling_layer(self, x):\n",
        "        return F.max_pool2d(x,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "cnn_model = CNNModel(num_classes=10)\n",
        "X_train_1=torch.from_numpy(X_train).float()\n",
        "Y_train=torch.from_numpy(train_labels).long()\n",
        "X_val_1=torch.from_numpy(X_val).float()\n",
        "Y_val=torch.from_numpy(val_labels).long()\n",
        "\n",
        "train_dataset = TensorDataset(X_train_1, Y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(cnn_model.parameters(),lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 377.8107\n",
            "Epoch 2, Loss: 252.2528\n",
            "Epoch 3, Loss: 219.0842\n",
            "Epoch 4, Loss: 196.9246\n",
            "Epoch 5, Loss: 180.6882\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    cnn_model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 90.39%\n"
          ]
        }
      ],
      "source": [
        "cnn_model.eval()\n",
        "outputs = cnn_model(X_val_1)\n",
        "_, predictions = torch.max(outputs, 1)\n",
        "accuracy = torch.sum(predictions == Y_val).item() / len(Y_val) * 100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Features Shape: torch.Size([48000, 3136])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    features_train = cnn_model(X_train_1, extract_features=True)\n",
        "    features_val = cnn_model(X_val_1, extract_features=True)\n",
        "\n",
        "print(\"Extracted Features Shape:\", features_train.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted features shape: torch.Size([48000, 3136])\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"Extracted features shape:\",features_train.shape)\n",
        "\n",
        "print(type(features_train))\n",
        "print(type(features_val))\n",
        "print(type(train_labels))\n",
        "print(type(val_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(48000, 3136)\n",
            "Epoch 1 of 30, Loss: 3282.1922\n",
            "Epoch 2 of 30, Loss: 1420.9801\n",
            "Epoch 3 of 30, Loss: 928.7048\n",
            "Epoch 4 of 30, Loss: 771.3124\n",
            "Epoch 5 of 30, Loss: 678.7016, Accuracy: 84.99%\n",
            "Epoch 6 of 30, Loss: 618.2691\n",
            "Epoch 7 of 30, Loss: 562.3737\n",
            "Epoch 8 of 30, Loss: 503.3449\n",
            "Epoch 9 of 30, Loss: 465.9221\n",
            "Epoch 10 of 30, Loss: 432.4650, Accuracy: 90.11%\n",
            "Epoch 11 of 30, Loss: 412.5715\n",
            "Epoch 12 of 30, Loss: 395.5066\n",
            "Epoch 13 of 30, Loss: 383.6852\n",
            "Epoch 14 of 30, Loss: 369.9119\n",
            "Epoch 15 of 30, Loss: 361.2814, Accuracy: 90.97%\n",
            "Epoch 16 of 30, Loss: 347.2040\n",
            "Epoch 17 of 30, Loss: 343.5088\n",
            "Epoch 18 of 30, Loss: 330.4753\n",
            "Epoch 19 of 30, Loss: 321.7928\n",
            "Epoch 20 of 30, Loss: 317.0097, Accuracy: 91.27%\n",
            "Epoch 21 of 30, Loss: 308.7700\n",
            "Epoch 22 of 30, Loss: 302.9335\n",
            "Epoch 23 of 30, Loss: 293.3309\n",
            "Epoch 24 of 30, Loss: 290.5275\n",
            "Epoch 25 of 30, Loss: 283.1849, Accuracy: 91.65%\n",
            "Epoch 26 of 30, Loss: 277.3105\n",
            "Epoch 27 of 30, Loss: 271.0014\n",
            "Epoch 28 of 30, Loss: 267.5697\n",
            "Epoch 29 of 30, Loss: 262.1845\n",
            "Epoch 30 of 30, Loss: 258.5151, Accuracy: 91.91%\n"
          ]
        }
      ],
      "source": [
        "np_1,np_2=features_train,features_val\n",
        "x_train,y_train,x_test,y_test=np_1.cpu().detach().numpy(),train_labels,np_2.cpu().detach().numpy(),val_labels\n",
        "x_train,x_test =x_train/x_train.max(),x_test/x_test.max()\n",
        "x_train=x_train.reshape(x_train.shape[0],-1)\n",
        "x_test=x_test.reshape(x_test.shape[0],-1)\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "def one_hot_encode(y,num_classes=10):\n",
        "    one_hot=np.zeros((y.shape[0],num_classes))\n",
        "    one_hot[np.arange(y.shape[0]),y]=1\n",
        "    return one_hot\n",
        "y_train=one_hot_encode(y_train)\n",
        "y_test=one_hot_encode(y_test)\n",
        "\n",
        "input_size=3136\n",
        "hidden_size1=128\n",
        "hidden_size2=64\n",
        "output_size=10\n",
        "learning_rate=0.03\n",
        "epochs=30\n",
        "batch_size=32\n",
        "dropout_rate=0.2\n",
        "\n",
        "np.random.seed(42)\n",
        "W1=np.random.randn(input_size,hidden_size1)*0.01\n",
        "b1=np.zeros((1,hidden_size1))\n",
        "W2=np.random.randn(hidden_size1,hidden_size2)*0.01\n",
        "b2=np.zeros((1,hidden_size2))\n",
        "W3=np.random.randn(hidden_size2,output_size)*0.01\n",
        "b3=np.zeros((1,output_size))\n",
        "\n",
        "# Activation functions\n",
        "def relu(Z):\n",
        "    return np.maximum(0,Z)\n",
        "\n",
        "def relu_derivative(Z):\n",
        "    return np.where(Z>0,1,0)\n",
        "\n",
        "def leaky_relu(Z,alpha=0.01):\n",
        "    return np.where(Z>0,Z,alpha*Z)\n",
        "\n",
        "def leaky_relu_derivative(Z,alpha=0.01):\n",
        "    return np.where(Z>0,1,alpha)\n",
        "\n",
        "def tanh(Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def tanh_derivative(Z):\n",
        "    return 1-np.tanh(Z)**2\n",
        "\n",
        "def gelu(Z):\n",
        "    return 0.5*Z*(1+np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3)))\n",
        "\n",
        "def gelu_derivative(Z):\n",
        "    return 0.5*(1+np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3)))+(Z*(1-np.tanh(np.sqrt(2/np.pi)*(Z+0.044715*Z**3))**2)*(np.sqrt(2/np.pi)*(1+3*0.044715*Z**2))*0.5)\n",
        "\n",
        "def softmax(Z):\n",
        "    expZ=np.exp(Z-np.max(Z,axis=1,keepdims=True))\n",
        "    return expZ/np.sum(expZ,axis=1,keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(Y_true,Y_pred):\n",
        "    return -np.mean(np.sum(Y_true*np.log(Y_pred+1e-9),axis=1))\n",
        "\n",
        "activation_function=relu\n",
        "activation_derivative=relu_derivative\n",
        "\n",
        "def forward_pass(X,dropout_rate=0.2):\n",
        "    global mask1,mask2\n",
        "    Z1=np.matmul(X,W1)+b1\n",
        "    A1=activation_function(Z1)\n",
        "    mask1=(np.random.rand(*A1.shape)>dropout_rate)\n",
        "    A1*=mask1\n",
        "    A1/=(1-dropout_rate)\n",
        "    Z2=np.matmul(A1,W2)+b2\n",
        "    A2=activation_function(Z2)\n",
        "    mask2=(np.random.rand(*A2.shape)>dropout_rate)\n",
        "    A2*=mask2\n",
        "    A2/=(1-dropout_rate)\n",
        "    Z3=np.matmul(A2,W3)+b3\n",
        "    A3=softmax(Z3)\n",
        "    return Z1,A1,Z2,A2,Z3,A3\n",
        "\n",
        "def backward_pass(X,Y,Z1,A1,Z2,A2,Z3,A3,dropout_rate=0.2):\n",
        "    global W1,b1,W2,b2,W3,b3\n",
        "    dZ3=A3-Y\n",
        "    dW3=np.matmul(A2.T,dZ3)/X.shape[0]\n",
        "    db3=np.mean(dZ3, axis=0, keepdims=True)\n",
        "    dA2=np.matmul(dZ3, W3.T)\n",
        "    dA2*=mask2\n",
        "    dA2/=(1-dropout_rate)\n",
        "    dZ2=dA2*activation_derivative(Z2)\n",
        "    dW2=np.matmul(A1.T,dZ2)/X.shape[0]\n",
        "    db2=np.mean(dZ2,axis=0,keepdims=True)\n",
        "    dA1=np.matmul(dZ2,W2.T)\n",
        "    dA1*=mask1\n",
        "    dA1/=(1-dropout_rate)\n",
        "    dZ1=dA1*activation_derivative(Z1)\n",
        "    dW1=np.matmul(X.T,dZ1)/X.shape[0]\n",
        "    db1=np.mean(dZ1,axis=0,keepdims=True)\n",
        "    W1-=learning_rate*dW1\n",
        "    b1-=learning_rate*db1\n",
        "    W2-=learning_rate*dW2\n",
        "    b2-=learning_rate*db2\n",
        "    W3-=learning_rate*dW3\n",
        "    b3-=learning_rate*db3\n",
        "\n",
        "loss_array=[]\n",
        "prev_loss=float('inf')\n",
        "accuracy_array=[]\n",
        "\n",
        "def predict(X):\n",
        "    _,_,_,_,_,A3=forward_pass(X,dropout_rate=0)\n",
        "    return np.argmax(A3,axis=1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    indices=np.random.permutation(x_train.shape[0])\n",
        "    x_train_shuffled=x_train[indices]\n",
        "    y_train_shuffled=y_train[indices]\n",
        "\n",
        "    epoch_loss=0\n",
        "    for i in range(0,x_train.shape[0],batch_size):\n",
        "        X_batch=x_train_shuffled[i:i+batch_size]\n",
        "        Y_batch=y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        Z1,A1,Z2,A2,Z3,A3=forward_pass(X_batch,dropout_rate)\n",
        "        loss=cross_entropy_loss(Y_batch,A3)\n",
        "        backward_pass(X_batch,Y_batch,Z1,A1,Z2,A2,Z3,A3,dropout_rate)\n",
        "        epoch_loss+=loss\n",
        "\n",
        "    loss_array.append(epoch_loss)\n",
        "    if epoch_loss>prev_loss:\n",
        "        learning_rate/=2\n",
        "        print(\"Learning decreased\")\n",
        "    prev_loss=epoch_loss\n",
        "    if (epoch+1)%5==0:\n",
        "        y_pred=predict(x_test)\n",
        "        y_true=np.argmax(y_test,axis=1)\n",
        "        accuracy=np.mean(y_pred==y_true)\n",
        "        accuracy_array.append(accuracy)\n",
        "        print(f\"Epoch {epoch+1} of {epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1} of {epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# ax1.plot(range(1, epochs + 1), loss_array, color='orange', linestyle='-', linewidth=1, label='Loss')\n",
        "# ax1.set_xlabel('Epochs')\n",
        "# ax1.set_ylabel('Loss', color='orange')\n",
        "# ax1.tick_params(axis='y')\n",
        "\n",
        "# ax2 = ax1.twinx()\n",
        "# ax2.plot(range(5, epochs + 1, 5), accuracy_array, color='blue', linestyle='--', linewidth=1, label='Accuracy')\n",
        "# ax2.set_ylabel('Accuracy', color='blue')\n",
        "# ax2.tick_params(axis='y')\n",
        "\n",
        "# plt.title('Training Loss and Test Accuracy')\n",
        "# fig.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
